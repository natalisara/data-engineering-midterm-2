## 1. **Redis-ის შესახებ**

### a) აღწერეთ Redis-ის 5 ძირითადი მონაცემთა ტიპი და მათი გამოყენების შემთხვევები.

Redis (Remote Dictionary Server) არის NoSQL მონაცემთა ბაზა, რომელიც ინახავს მონაცემებს ოპერატიულ მეხსიერებაში (RAM). მისი ძირითადი მონაცემთა ტიპებია:

1. Strings

   - ყველაზე ელემენტარული ტიპი, სადაც ინახება ტექსტი ან ბინარული მონაცემები.
   - გამოყენება: ქეში (მაგალითად, მომხმარებლის სესიის ტოკენები), დათვლადი მონაცემები (მაგ. გვერდების ნახვების რაოდენობა `INCR` ბრძანებით).

2. Lists

   - ელემენტების კოლექცია, რომელიც ინახება მიმდევრობით.
   - გამოყენება: რიგის (queue) იმპლემენტაცია, სიახლეების ან შეტყობინებების შენახვა (მაგალითად, ჩატის ისტორია `LPUSH`/`LRANGE`).

3. Sets

   - უნიკალური ელემენტების არაორდინირებული კოლექცია.
   - გამოყენება: დუპლიკატების ამოშლა, უნიკალური მომხმარებლების გასაგებად (მაგალითად, დღეში აქტიური მომხმარებლების სია `SADD`).

4. Hashes

   - Key-Value წყვილების ჯგუფი ერთ ობიექტში.
   - გამოყენება: მომხმარებლის პროფილის მონაცემების შენახვა (მაგალითად, `HSET user:1001 name "John Doe"`).

5. Sorted Sets

   - ელემენტები ინახება ქულების (scores) შესაბამისად დალაგებულად.
   - გამოყენება: რეიტინგული სისტემები, ლიდერბორდები (მაგალითად, `ZADD leaderboard 100 "player1"`).

### b) განმარტეთ, როგორ უზრუნველყოფს Redis მონაცემთა მდგრადობას (persistence).

Redis მონაცემთა მდგრადობის უზრუნველსაყოფად იყენებს რამდენიმე მექანიზმს:

1. **RDB (Redis Database File)** - მონაცემების Snapshot-ის შექმნა გარკვეულ ინტერვალებში და შენახვა დისკზე.
2. **AOF (Append Only File)** - ყველა ბრძანება ინახება ლოგ ფაილში.
3. **Hybrid Mode (RDB + AOF)** - ორივე მეთოდის გაერთიანება, რაც უზრუნველყოფს როგორც სწრაფად დატვირთვას, ისე მონაცემთა მდგრადობას.

---

## 2. **Apache Kafka-ს შესახებ**

### a) აღწერეთ Kafka-ს ძირითადი კომპონენტები (brokers, topics, partitions, consumer groups).

Kafka შედგება რამდენიმე ძირითადი კომპონენტისგან:

- **Brokers** – დამოუკიდებელი სერვერები, რომლებიც ინახავენ და მართავენ შეტყობინებებს.
- **Topics** – ლოგიკური კატეგორიები, რომლებშიც შეტყობინებები ინახება.
- **Partitions** – ქვედანაყოფები, რომლებიც უზრუნველყოფენ მასშტაბირებადობას და პარალელურ დამუშავებას.
- **Consumer Groups** – მომხმარებელთა ჯგუფები, რომლებიც კითხულობენ შეტყობინებებს კონკრეტული ტოპიკიდან და ინაწილებენ მათ შორის დატვირთვას.

### b) როგორ უზრუნველყოფს Kafka მასშტაბირებადობას და მაღალ წარმადობას?

Kafka-ს მასშტაბირებადობა და წარმადობა უზრუნველყოფილია რამდენიმე ფაქტორით:

- **პარტიციების განაწილება სხვადასხვა ბროკერებზე**, რაც უზრუნველყოფს პარალელურ დამუშავებას.
- **ბინარული პროტოკოლი** – იყენებს მაღალი წარმადობის ბინარულ პროტოკოლს, რომელიც ამცირებს მონაცემთა გადაცემის დროს.
- **ზედმეტი რესურსის ხარჯვის თავიდან აცილება** – მომხმარებლებს შეუძლიათ შეტყობინებების პირდაპირი წაკითხვა, რაც სისტემას ნაკლებ დატვირთვას აყენებს.

---

## 3. **Apache Airflow-ს შესახებ**

### a) რა არის DAG? ახსენით მისი ძირითადი მახასიათებლები.

DAG (Directed Acyclic Graph) წარმოადგენს ამოცანების (tasks) მართვის სტრუქტურას, რომელიც განსაზღვრავს მათი შესრულების თანამიმდევრობას. DAG-ის მახასიათებლებია:

- **დამოკიდებულება** – თითოეული დავალება დამოკიდებულია სხვაზე.
- **ციკლების არარსებობა** – DAG-ს არ აქვს ციკლები, რაც უზრუნველყოფს სწორი მიმდევრობით შესრულებას.
- **დაგეგმვა და მონიტორინგი** – Airflow საშუალებას გაძლევთ აკონტროლოთ და მართოთ დავალებების შესრულება.

### b) განასხვავეთ Airflow-ს Operators და Sensors ერთმანეთისგან.

- **Operators** – მოქმედებები, რომლებიც ასრულებენ დავალებებს (მაგალითად, `PythonOperator`, `BashOperator`).
- **Sensors** – მოლოდინის რეჟიმში მყოფი ოპერატორები, რომლებიც ამოწმებენ რაიმე პირობას (მაგალითად, `FileSensor`, რომელიც ამოწმებს ფაილის არსებობას).

---

## 4. **ETL vs ELT**

### a) შეადარეთ ეს ორი მიდგომა – რა უპირატესობები და ნაკლოვანებები აქვს თითოეულს?

ETL (Extract, Transform, Load) მონაცემთა დამუშავების ტრადიციული მიდგომაა, სადაც მონაცემები ჯერ მუშავდება და შემდეგ იტვირთება მონაცემთა საწყობში. ეს უზრუნველყოფს მონაცემების სტრუქტურირებულ და კონსისტენტურ ფორმატს, მაგრამ ზოგჯერ სიჩქარის ხარჯზე.

ELT (Extract, Load, Transform) პროცესში მონაცემები პირდაპირ იტვირთება საცავში და ტრანსფორმაცია ხდება მოგვიანებით. ეს მიდგომა განსაკუთრებით ეფექტურია დიდი მონაცემთა ნაკადებისთვის, თუმცა შეიძლება დროებით არასტაბილურობა გამოიწვიოს.

### b) მოიყვანეთ კონკრეტული შემთხვევები, როდესაც ერთი მეორეზე უკეთესია.

ETL საუკეთესოა ისეთი სისტემებისთვის, სადაც მონაცემთა მაღალი ხარისხი და სიზუსტე კრიტიკულია, მაგალითად, ფინანსური ანგარიშგება. ELT კი უფრო სასარგებლოა მონაცემთა ანალიტიკისა და Machine Learning პროცესებში, სადაც მონაცემთა დიდი მოცულობა და მოქნილობა მნიშვნელოვანია.

---

## 5. **მონაცემთა შენახვის კონცეფციები**

### a) აღწერეთ განსხვავებები Data Lake, Data Warehouse და Data Mart-ს შორის.

**Data Lake** ინახავს ნედლ მონაცემებს და გამოიყენება Big Data ანალიტიკისთვის.

**Data Warehouse** არის სტრუქტურირებული მონაცემთა საცავი, რომელიც განკუთვნილია ბიზნეს ანალიტიკისთვის.

**Data Mart** წარმოადგენს Data Warehouse-ის მცირე ვერსიას, რომელიც ოპტიმიზებულია კონკრეტული განყოფილების საჭიროებისთვის.

### b) როგორ უკავშირდება თითოეული ETL/ELT პროცესებს?

**Data Lake** ძირითადად ეფუძნება ELT-ს, რადგან მონაცემები ნედლი სახით იტვირთება.

**Data Warehouse** ემყარება ETL-ს, რადგან მონაცემები წინასწარ მუშავდება.

**Data Mart** ძირითადად ETL პროცესს იყენებს, რათა ბიზნესისთვის ოპტიმიზირებული მონაცემები უზრუნველყოს.

